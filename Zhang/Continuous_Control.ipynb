{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from collections import deque\n",
    "from skimage.io import imsave\n",
    "\n",
    "import pdb\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "from baselines.common.running_mean_std import RunningMeanStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "path = 'C:\\\\Users\\\\mbgpcsk4\\\\Dropbox (The University of Manchester)\\\\D2019\\\\University\\\\Udacity\\\\P2_Reacher_Submission\\\\Reacher_Windows_x86_64\\\\Reacher.exe'\n",
    "\n",
    "env = UnityEnvironment(file_name=path)\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseAgent:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.task_ind = 0\n",
    "\n",
    "    def close(self):\n",
    "        close_obj(self.task)\n",
    "\n",
    "    def save(self, filename):\n",
    "        torch.save(self.network.state_dict(), '%s.model' % (filename))\n",
    "        with open('%s.stats' % (filename), 'wb') as f:\n",
    "            pickle.dump(self.config.state_normalizer.state_dict(), f)\n",
    "\n",
    "    def load(self, filename):\n",
    "        state_dict = torch.load('%s.model' % filename, map_location=lambda storage, loc: storage)\n",
    "        self.network.load_state_dict(state_dict)\n",
    "        with open('%s.stats' % (filename), 'rb') as f:\n",
    "            self.config.state_normalizer.load_state_dict(pickle.load(f))\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def eval_episode(self):\n",
    "        env = self.config.eval_env\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            action = self.eval_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            if ret is not None:\n",
    "                break\n",
    "        return ret\n",
    "\n",
    "    def eval_episodes(self):\n",
    "        episodic_returns = []\n",
    "        for ep in range(self.config.eval_episodes):\n",
    "            total_rewards = self.eval_episode()\n",
    "            episodic_returns.append(np.sum(total_rewards))\n",
    "        return {\n",
    "            'episodic_return_test': np.mean(episodic_returns),\n",
    "        }\n",
    "\n",
    "\n",
    "    def switch_task(self):\n",
    "        config = self.config\n",
    "        if not config.tasks:\n",
    "            return\n",
    "        segs = np.linspace(0, config.max_steps, len(config.tasks) + 1)\n",
    "        if self.total_steps > segs[self.task_ind + 1]:\n",
    "            self.task_ind += 1\n",
    "            self.task = config.tasks[self.task_ind]\n",
    "            self.states = self.task.reset()\n",
    "            self.states = config.state_normalizer(self.states)\n",
    "\n",
    "    def record_episode(self, dir, env):\n",
    "        mkdir(dir)\n",
    "        steps = 0\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            self.record_obs(env, dir, steps)\n",
    "            action = self.record_step(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ret = info[0]['episodic_return']\n",
    "            steps += 1\n",
    "            if ret is not None:\n",
    "                break\n",
    "\n",
    "    def record_step(self, state):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # For DMControl\n",
    "    def record_obs(self, env, dir, steps):\n",
    "        env = env.env.envs[0]\n",
    "        obs = env.render(mode='rgb_array')\n",
    "        imsave('%s/%04d.png' % (dir, steps), obs)\n",
    "\n",
    "\n",
    "class BaseActor(mp.Process):\n",
    "    STEP = 0\n",
    "    RESET = 1\n",
    "    EXIT = 2\n",
    "    SPECS = 3\n",
    "    NETWORK = 4\n",
    "    CACHE = 5\n",
    "\n",
    "    def __init__(self, config):\n",
    "        mp.Process.__init__(self)\n",
    "        self.config = config\n",
    "        self.__pipe, self.__worker_pipe = mp.Pipe()\n",
    "\n",
    "        self._state = None\n",
    "        self._task = None\n",
    "        self._network = None\n",
    "        self._total_steps = 0\n",
    "        self.__cache_len = 2\n",
    "\n",
    "        if not config.async_actor:\n",
    "            self.start = lambda: None\n",
    "            self.step = self._sample\n",
    "            self.close = lambda: None\n",
    "            self._set_up()\n",
    "            self._task = config.task_fn()\n",
    "\n",
    "    def _sample(self):\n",
    "        transitions = []\n",
    "        for _ in range(self.config.sgd_update_frequency):\n",
    "            transitions.append(self._transition())\n",
    "        return transitions\n",
    "\n",
    "    def run(self):\n",
    "        self._set_up()\n",
    "        config = self.config\n",
    "        self._task = config.task_fn()\n",
    "\n",
    "        cache = deque([], maxlen=2)\n",
    "        while True:\n",
    "            op, data = self.__worker_pipe.recv()\n",
    "            if op == self.STEP:\n",
    "                if not len(cache):\n",
    "                    cache.append(self._sample())\n",
    "                    cache.append(self._sample())\n",
    "                self.__worker_pipe.send(cache.popleft())\n",
    "                cache.append(self._sample())\n",
    "            elif op == self.EXIT:\n",
    "                self.__worker_pipe.close()\n",
    "                return\n",
    "            elif op == self.NETWORK:\n",
    "                self._network = data\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def _transition(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _set_up(self):\n",
    "        pass\n",
    "\n",
    "    def step(self):\n",
    "        self.__pipe.send([self.STEP, None])\n",
    "        return self.__pipe.recv()\n",
    "\n",
    "    def close(self):\n",
    "        self.__pipe.send([self.EXIT, None])\n",
    "        self.__pipe.close()\n",
    "\n",
    "    def set_network(self, net):\n",
    "        if not self.config.async_actor:\n",
    "            self._network = net\n",
    "        else:\n",
    "            self.__pipe.send([self.NETWORK, net])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DDPGAgent(BaseAgent):\n",
    "    def __init__(self, config):\n",
    "        BaseAgent.__init__(self, config)\n",
    "        self.config = config\n",
    "        self.network = config.network_fn()\n",
    "        self.target_network = config.network_fn()\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "        self.replay = config.replay_fn()\n",
    "        self.random_process = config.random_process_fn()\n",
    "        self.total_steps = 0\n",
    "        self.state = None\n",
    "\n",
    "    def soft_update(self, target, src):\n",
    "        for target_param, param in zip(target.parameters(), src.parameters()):\n",
    "            target_param.detach_()\n",
    "            target_param.copy_(target_param * (1.0 - self.config.target_network_mix) +\n",
    "                               param * self.config.target_network_mix)\n",
    "\n",
    "    def eval_step(self, state):\n",
    "        self.config.state_normalizer.set_read_only()\n",
    "        state = self.config.state_normalizer(state)\n",
    "        action = self.network(state)\n",
    "        self.config.state_normalizer.unset_read_only()\n",
    "        return to_np(action)\n",
    "\n",
    "    def step(self):\n",
    "        config = self.config\n",
    "        if self.state is None:\n",
    "            self.random_process.reset_states()\n",
    "            self.state = self.task.reset()\n",
    "            self.state = config.state_normalizer(self.state)\n",
    "\n",
    "        if self.total_steps < config.warm_up:\n",
    "            action = [self.task.action_space.sample()]\n",
    "        else:\n",
    "            action = self.network(self.state)\n",
    "            action = to_np(action)\n",
    "            action += self.random_process.sample()\n",
    "        action = np.clip(action, self.task.action_space.low, self.task.action_space.high)\n",
    "        next_state, reward, done, info = self.task.step(action)\n",
    "        next_state = self.config.state_normalizer(next_state)\n",
    "        reward = self.config.reward_normalizer(reward)\n",
    "\n",
    "        experiences = list(zip(self.state, action, reward, next_state, done))\n",
    "        self.replay.feed_batch(experiences)\n",
    "        if done[0]:\n",
    "            self.random_process.reset_states()\n",
    "        self.state = next_state\n",
    "        self.total_steps += 1\n",
    "\n",
    "        if self.replay.size() >= config.warm_up:\n",
    "            experiences = self.replay.sample()\n",
    "            states, actions, rewards, next_states, terminals = experiences\n",
    "            states = tensor(states)\n",
    "            actions = tensor(actions)\n",
    "            rewards = tensor(rewards).unsqueeze(-1)\n",
    "            next_states = tensor(next_states)\n",
    "            mask = tensor(1 - terminals).unsqueeze(-1)\n",
    "\n",
    "            phi_next = self.target_network.feature(next_states)\n",
    "            a_next = self.target_network.actor(phi_next)\n",
    "            q_next = self.target_network.critic(phi_next, a_next)\n",
    "            q_next = config.discount * mask * q_next\n",
    "            q_next.add_(rewards)\n",
    "            q_next = q_next.detach()\n",
    "            phi = self.network.feature(states)\n",
    "            q = self.network.critic(phi, actions)\n",
    "            critic_loss = (q - q_next).pow(2).mul(0.5).sum(-1).mean()\n",
    "\n",
    "            self.network.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.network.critic_opt.step()\n",
    "\n",
    "            phi = self.network.feature(states)\n",
    "            action = self.network.actor(phi)\n",
    "            policy_loss = -self.network.critic(phi.detach(), action).mean()\n",
    "\n",
    "            self.network.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.network.actor_opt.step()\n",
    "\n",
    "            self.soft_update(self.target_network, self.network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Replay:\n",
    "    def __init__(self, memory_size, batch_size, drop_prob=0, to_np=True):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.data = []\n",
    "        self.pos = 0\n",
    "        self.drop_prob = drop_prob\n",
    "        self.to_np = to_np\n",
    "\n",
    "    def feed(self, experience):\n",
    "        if np.random.rand() < self.drop_prob:\n",
    "            return\n",
    "        if self.pos >= len(self.data):\n",
    "            self.data.append(experience)\n",
    "        else:\n",
    "            self.data[self.pos] = experience\n",
    "        self.pos = (self.pos + 1) % self.memory_size\n",
    "\n",
    "    def feed_batch(self, experience):\n",
    "        for exp in experience:\n",
    "            self.feed(exp)\n",
    "\n",
    "    def sample(self, batch_size=None):\n",
    "        if self.empty():\n",
    "            return None\n",
    "        if batch_size is None:\n",
    "            batch_size = self.batch_size\n",
    "\n",
    "        sampled_indices = [np.random.randint(0, len(self.data)) for _ in range(batch_size)]\n",
    "        sampled_data = [self.data[ind] for ind in sampled_indices]\n",
    "        sampled_data = zip(*sampled_data)\n",
    "        if self.to_np:\n",
    "            sampled_data = list(map(lambda x: np.asarray(x), sampled_data))\n",
    "        return sampled_data\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def empty(self):\n",
    "        return not len(self.data)\n",
    "\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.data)\n",
    "\n",
    "    def clear(self):\n",
    "        self.data = []\n",
    "        self.pos = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "def layer_init(layer, w_scale=1.0):\n",
    "    nn.init.orthogonal_(layer.weight.data)\n",
    "    layer.weight.data.mul_(w_scale)\n",
    "    nn.init.constant_(layer.bias.data, 0)\n",
    "    return layer\n",
    "\n",
    "class DummyBody(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(DummyBody, self).__init__()\n",
    "        self.feature_dim = state_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeterministicActorCriticNet(nn.Module, BaseNet):\n",
    "    def __init__(self,\n",
    "                 state_dim,\n",
    "                 action_dim,\n",
    "                 actor_opt_fn,\n",
    "                 critic_opt_fn,\n",
    "                 phi_body=None,\n",
    "                 actor_body=None,\n",
    "                 critic_body=None):\n",
    "        super(DeterministicActorCriticNet, self).__init__()\n",
    "        if phi_body is None: phi_body = DummyBody(state_dim)\n",
    "        if actor_body is None: actor_body = DummyBody(phi_body.feature_dim)\n",
    "        if critic_body is None: critic_body = DummyBody(phi_body.feature_dim)\n",
    "        self.phi_body = phi_body\n",
    "        self.actor_body = actor_body\n",
    "        self.critic_body = critic_body\n",
    "        self.fc_action = layer_init(nn.Linear(actor_body.feature_dim, action_dim), 1e-3)\n",
    "        self.fc_critic = layer_init(nn.Linear(critic_body.feature_dim, 1), 1e-3)\n",
    "\n",
    "        self.actor_params = list(self.actor_body.parameters()) + list(self.fc_action.parameters())\n",
    "        self.critic_params = list(self.critic_body.parameters()) + list(self.fc_critic.parameters())\n",
    "        self.phi_params = list(self.phi_body.parameters())\n",
    "        \n",
    "        self.actor_opt = actor_opt_fn(self.actor_params + self.phi_params)\n",
    "        self.critic_opt = critic_opt_fn(self.critic_params + self.phi_params)\n",
    "        self.to(Config.DEVICE)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        phi = self.feature(obs)\n",
    "        action = self.actor(phi)\n",
    "        return action\n",
    "\n",
    "    def feature(self, obs):\n",
    "        obs = tensor(obs)\n",
    "        return self.phi_body(obs)\n",
    "\n",
    "    def actor(self, phi):\n",
    "        return torch.tanh(self.fc_action(self.actor_body(phi)))\n",
    "\n",
    "    def critic(self, phi, a):\n",
    "        return self.fc_critic(self.critic_body(phi, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCBody(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(FCBody, self).__init__()\n",
    "        dims = (state_dim,) + hidden_units\n",
    "        self.layers = nn.ModuleList(\n",
    "            [layer_init(nn.Linear(dim_in, dim_out)) for dim_in, dim_out in zip(dims[:-1], dims[1:])])\n",
    "        self.gate = gate\n",
    "        self.feature_dim = dims[-1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.gate(layer(x))\n",
    "        return x\n",
    "    \n",
    "class TwoLayerFCBodyWithAction(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_units=(64, 64), gate=F.relu):\n",
    "        super(TwoLayerFCBodyWithAction, self).__init__()\n",
    "        hidden_size1, hidden_size2 = hidden_units\n",
    "        self.fc1 = layer_init(nn.Linear(state_dim, hidden_size1))\n",
    "        self.fc2 = layer_init(nn.Linear(hidden_size1 + action_dim, hidden_size2))\n",
    "        self.gate = gate\n",
    "        self.feature_dim = hidden_size2\n",
    "\n",
    "    def forward(self, x, action):\n",
    "        x = self.gate(self.fc1(x))\n",
    "        phi = self.gate(self.fc2(torch.cat([x, action], dim=1)))\n",
    "        return phi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseNormalizer:\n",
    "    def __init__(self, read_only=False):\n",
    "        self.read_only = read_only\n",
    "\n",
    "    def set_read_only(self):\n",
    "        self.read_only = True\n",
    "\n",
    "    def unset_read_only(self):\n",
    "        self.read_only = False\n",
    "\n",
    "    def state_dict(self):\n",
    "        return None\n",
    "\n",
    "    def load_state_dict(self, _):\n",
    "        return\n",
    "\n",
    "\n",
    "class MeanStdNormalizer(BaseNormalizer):\n",
    "    def __init__(self, read_only=False, clip=10.0, epsilon=1e-8):\n",
    "        BaseNormalizer.__init__(self, read_only)\n",
    "        self.read_only = read_only\n",
    "        self.rms = None\n",
    "        self.clip = clip\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = np.asarray(x)\n",
    "        if self.rms is None:\n",
    "            self.rms = RunningMeanStd(shape=(1,) + x.shape[1:])\n",
    "        if not self.read_only:\n",
    "            self.rms.update(x)\n",
    "        return np.clip((x - self.rms.mean) / np.sqrt(self.rms.var + self.epsilon),\n",
    "                       -self.clip, self.clip)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.rms.mean,\n",
    "                'var': self.rms.var}\n",
    "\n",
    "    def load_state_dict(self, saved):\n",
    "        self.rms.mean = saved['mean']\n",
    "        self.rms.var = saved['var']\n",
    "\n",
    "class RescaleNormalizer(BaseNormalizer):\n",
    "    def __init__(self, coef=1.0):\n",
    "        BaseNormalizer.__init__(self)\n",
    "        self.coef = coef\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = np.asarray(x)\n",
    "        return self.coef * x\n",
    "\n",
    "\n",
    "class ImageNormalizer(RescaleNormalizer):\n",
    "    def __init__(self):\n",
    "        RescaleNormalizer.__init__(self, 1.0 / 255)\n",
    "\n",
    "\n",
    "class SignNormalizer(BaseNormalizer):\n",
    "    def __call__(self, x):\n",
    "        return np.sign(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Config:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.parser = argparse.ArgumentParser()\n",
    "        self.task_fn = None\n",
    "        self.optimizer_fn = None\n",
    "        self.actor_optimizer_fn = None\n",
    "        self.critic_optimizer_fn = None\n",
    "        self.network_fn = None\n",
    "        self.actor_network_fn = None\n",
    "        self.critic_network_fn = None\n",
    "        self.replay_fn = None\n",
    "        self.random_process_fn = None\n",
    "        self.discount = None\n",
    "        self.target_network_update_freq = None\n",
    "        self.exploration_steps = None\n",
    "        self.log_level = 0\n",
    "        self.history_length = None\n",
    "        self.double_q = False\n",
    "        self.tag = 'vanilla'\n",
    "        self.num_workers = 1\n",
    "        self.gradient_clip = None\n",
    "        self.entropy_weight = 0\n",
    "        self.use_gae = False\n",
    "        self.gae_tau = 1.0\n",
    "        self.target_network_mix = 0.001\n",
    "        self.state_normalizer = RescaleNormalizer()\n",
    "        self.reward_normalizer = RescaleNormalizer()\n",
    "        self.min_memory_size = None\n",
    "        self.max_steps = 0\n",
    "        self.rollout_length = None\n",
    "        self.value_loss_weight = 1.0\n",
    "        self.iteration_log_interval = 30\n",
    "        self.categorical_v_min = None\n",
    "        self.categorical_v_max = None\n",
    "        self.categorical_n_atoms = 51\n",
    "        self.num_quantiles = None\n",
    "        self.optimization_epochs = 4\n",
    "        self.mini_batch_size = 64\n",
    "        self.termination_regularizer = 0\n",
    "        self.sgd_update_frequency = None\n",
    "        self.random_action_prob = None\n",
    "        self.__eval_env = None\n",
    "        self.log_interval = int(1e3)\n",
    "        self.save_interval = 0\n",
    "        self.eval_interval = 0\n",
    "        self.eval_episodes = 10\n",
    "        self.async_actor = True\n",
    "        self.tasks = False\n",
    "\n",
    "    @property\n",
    "    def eval_env(self):\n",
    "        return self.__eval_env\n",
    "\n",
    "    @eval_env.setter\n",
    "    def eval_env(self, env):\n",
    "        self.__eval_env = env\n",
    "        self.state_dim = env.state_dim\n",
    "        self.action_dim = env.action_dim\n",
    "        self.task_name = env.name\n",
    "\n",
    "    def add_argument(self, *args, **kwargs):\n",
    "        self.parser.add_argument(*args, **kwargs)\n",
    "\n",
    "    def merge(self, config_dict=None):\n",
    "        if config_dict is None:\n",
    "            args = self.parser.parse_args()\n",
    "            config_dict = args.__dict__\n",
    "        for key in config_dict.keys():\n",
    "            setattr(self, key, config_dict[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self,\n",
    "                 name,\n",
    "                 num_envs=1,\n",
    "                 single_process=True,\n",
    "                 log_dir=None,\n",
    "                 episode_life=True,\n",
    "                 seed=np.random.randint(int(1e5))):\n",
    "        if log_dir is not None:\n",
    "            mkdir(log_dir)\n",
    "        envs = [make_env(name, seed, i, episode_life) for i in range(num_envs)]\n",
    "        if single_process:\n",
    "            Wrapper = DummyVecEnv\n",
    "        else:\n",
    "            Wrapper = SubprocVecEnv\n",
    "        self.env = Wrapper(envs)\n",
    "        self.name = name\n",
    "        self.observation_space = self.env.observation_space\n",
    "        self.state_dim = int(np.prod(self.env.observation_space.shape))\n",
    "\n",
    "        self.action_space = self.env.action_space\n",
    "        if isinstance(self.action_space, Discrete):\n",
    "            self.action_dim = self.action_space.n\n",
    "        elif isinstance(self.action_space, Box):\n",
    "            self.action_dim = self.action_space.shape[0]\n",
    "        else:\n",
    "            assert 'unknown action space'\n",
    "\n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "\n",
    "    def step(self, actions):\n",
    "        if isinstance(self.action_space, Box):\n",
    "            actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
    "        return self.env.step(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomProcess(object):\n",
    "    def reset_states(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class OrnsteinUhlenbeckProcess(RandomProcess):\n",
    "    def __init__(self, size, std, theta=.15, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = 0\n",
    "        self.std = std\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.std() * np.sqrt(\n",
    "            self.dt) * np.random.randn(*self.size)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_steps(agent):\n",
    "    config = agent.config\n",
    "    agent_name = agent.__class__.__name__\n",
    "    t0 = time.time()\n",
    "    while True:\n",
    "        if config.save_interval and not agent.total_steps % config.save_interval:\n",
    "            agent.save('data/%s-%s-%d' % (agent_name, config.tag, agent.total_steps))\n",
    "        if config.log_interval and not agent.total_steps % config.log_interval:\n",
    "            t0 = time.time()\n",
    "        if config.eval_interval and not agent.total_steps % config.eval_interval:\n",
    "            agent.eval_episodes()\n",
    "        if config.max_steps and agent.total_steps >= config.max_steps:\n",
    "            agent.close()\n",
    "            break\n",
    "        agent.step()\n",
    "        agent.switch_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearSchedule' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-fae3f4433c01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarm_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_network_mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mrun_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDDPGAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-d21a11618785>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_process\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_process_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-fae3f4433c01>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n\u001b[1;32m---> 24\u001b[1;33m     size=(config.action_dim,), std=LinearSchedule(0.2))\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarm_up\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_network_mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LinearSchedule' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "config.num_agents = len(env_info.agents)\n",
    "config.action_dim = brain.vector_action_space_size\n",
    "config.state_dim = int(33)\n",
    "\n",
    "\n",
    "config.max_steps = int(1e6)\n",
    "config.eval_interval = int(1e4)\n",
    "config.eval_episodes = 20\n",
    "\n",
    "config.network_fn = lambda: DeterministicActorCriticNet(\n",
    "    config.state_dim, config.action_dim,\n",
    "    actor_body=FCBody(config.state_dim, (400, 300), gate=F.relu),\n",
    "    critic_body=TwoLayerFCBodyWithAction(\n",
    "        config.state_dim, config.action_dim, (400, 300), gate=F.relu),\n",
    "    actor_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-4),\n",
    "    critic_opt_fn=lambda params: torch.optim.Adam(params, lr=1e-3))\n",
    "\n",
    "config.replay_fn = lambda: Replay(memory_size=int(1e6), batch_size=64)\n",
    "config.discount = 0.99\n",
    "config.random_process_fn = lambda: OrnsteinUhlenbeckProcess(\n",
    "    size=(config.action_dim,), std=LinearSchedule(0.2))\n",
    "config.warm_up = int(1e4)\n",
    "config.target_network_mix = 1e-3\n",
    "run_steps(DDPGAgent(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'num_agents' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-2530439ff707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrain_name\u001b[0m\u001b[1;33m]\u001b[0m     \u001b[1;31m# reset the environment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_observations\u001b[0m                  \u001b[1;31m# get the current state (for each agent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m                          \u001b[1;31m# initialize the score (for each agent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# select an action (for each agent)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'num_agents' is not defined"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
